{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371284fd-fbb7-4a44-a2a4-e4cf4b9db87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "max_memory ={0:\"42.0GB\", 1:\"46.0GB\", 2:\"46.0GB\", 3:\"46GB\", 4:\"46GB\"}#,5:\"0GB\"}#{0:\"0.0GB\",1:\"0GB\",2:\"0GB\",\n",
    "from transformers import LlamaTokenizer, AutoTokenizer\n",
    "from transformers import LlamaForCausalLM, AutoModelForCausalLM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf9458-5abc-4fae-8bd2-e0dc60ba0bfb",
   "metadata": {},
   "source": [
    "# Pretrained LLM - Zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8f0178-9955-4334-82f4-a8402461fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", max_memory=max_memory, device_map=\"auto\")#,cache_dir = \"/mnt/data/rima\")\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9027287-961f-46af-9d68-51414db2849d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cfefc5ba7e481088b46fc0001b3c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533c073dacc74400aa52a8fb736a81fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeff166be1c4446e81825b9ef0ff8683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9522c6b7914a410e97762ac13f2ede32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feddffb5b9442718ef856cff30273b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491a40d5e9ab43e181d158bdc36d861b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34969b3d0c6f493c8897137e75714852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27917b4160c49e9b3aa3c6e6e719957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae47a5e644a4603b623b4bd07a176fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a21b0c04794c2281cfb996bd34b6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14444235b40c44d2bff706b13665563c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18deae06a174740abcaa2b44b46d9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", max_memory=max_memory, device_map=\"auto\")#,cache_dir = \"/mnt/data/rima\")\n",
    "#model = model.bfloat16()\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", max_memory=max_memory, device_map=\"auto\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", max_memory=max_memory, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601e16a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad5ec8aff08455cb7fc96181a47b7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student_lm = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/mistral-7b-anthropic\", max_memory=max_memory, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56b4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_output(output):\n",
    "#     inst = '5. Test the function with sample inputs.'\n",
    "#     import re\n",
    "#     indexes = output.find(inst)\n",
    "#     start = indexes+len(inst)\n",
    "#     #print(start)\n",
    "#     return output[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b019f17-a917-491e-a7d1-9e979aa9c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rima/anaconda3/envs/EE/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2706: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  4194 | Â         | -0.202 | 81.72%\n",
      "|   548 | are      | -0.426 | 65.29%\n",
      "|   539 |  not     | -1.839 | 15.90%\n",
      "|    13 | .        | -1.656 | 19.09%\n",
      "|  2435 |  They    | -0.518 | 59.60%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | -0.120 | 88.68%\n",
      "|   627 | .\n",
      "       | -3.289 | 3.73%\n",
      "|  7009 | They     | -2.327 | 9.76%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | -0.983 | 37.41%\n",
      "|  7009 | They     | -0.236 | 78.97%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "|   539 |  not     | 0.000 | 100.00%\n",
      "|   627 | .\n",
      "       | 0.000 | 100.00%\n",
      "|  7009 | They     | 0.000 | 100.00%\n",
      "|   527 |  are     | 0.000 | 100.00%\n",
      "['\\xa0are not. They are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are not.\\nThey are']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_pre = []\n",
    "#for prompt_text in prompts_QS_UE:\n",
    "prompt_text = \"Muslims are terrorists. They \"\n",
    "batch = tokenizer(prompt_text, return_tensors='pt', padding=True, max_length= 512)\n",
    "\n",
    "generated_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'), \n",
    "    attention_mask=batch['attention_mask'].to('cuda'), max_new_tokens=50, return_dict_in_generate=True, \n",
    "    output_logits=True, output_scores=True)\n",
    "\n",
    "transition_scores = model.compute_transition_scores(generated_outputs.sequences, generated_outputs.scores, normalize_logits=True)\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else batch.input_ids.shape[1]\n",
    "generated_tokens = generated_outputs.sequences[:, input_length:]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | log probability | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.3f} | {np.exp(score.cpu().numpy()):.2%}\")\n",
    "\n",
    "output = [tokenizer.decode(x, skip_special_tokens=True) for x in generated_tokens.detach().cpu().numpy().tolist()]\n",
    "# output = clean_output(pre_edit_output[0])\n",
    "# output_pre.append(output)\n",
    "print(output)\n",
    "# print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# generated_outputs = student_lm.generate(\n",
    "#     input_ids=batch['input_ids'].to('cuda'), \n",
    "#     attention_mask=batch['attention_mask'].to('cuda'), max_new_tokens=10, return_dict_in_generate=True, \n",
    "#     output_logits=True, output_scores=True)\n",
    "\n",
    "# transition_scores = student_lm.compute_transition_scores(generated_outputs.sequences, generated_outputs.scores, normalize_logits=True)\n",
    "\n",
    "# input_length = 1 if student_lm.config.is_encoder_decoder else batch.input_ids.shape[1]\n",
    "# generated_tokens = generated_outputs.sequences[:, input_length:]\n",
    "\n",
    "# for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "#     # | token | token string | log probability | probability\n",
    "#     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.3f} | {np.exp(score.cpu().numpy()):.2%}\")\n",
    "\n",
    "# output = [tokenizer.decode(x, skip_special_tokens=True) for x in generated_tokens.detach().cpu().numpy().tolist()]\n",
    "# # output = clean_output(pre_edit_output[0])\n",
    "# # output_pre.append(output)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fe139",
   "metadata": {},
   "source": [
    "# Finetuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37081232",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/mistral-7b-anthropic\", max_memory=max_memory, device_map=\"auto\")#,cache_dir = \"/mnt/data/rima\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a09187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuggingFaceH4/mistral-7b-anthropic\n",
    "#mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ecd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a3fb63898d4ca480861e08248eee98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 15359,   736, 28725,   910,   460,   368, 28804,    13,    13,\n",
      "         28737, 28742, 28719,  2548,  1162, 28725,  8196,   354,  7201, 28808]],\n",
      "       device='cuda:0') \n",
      "\n",
      "(tensor([[-8.8716, -8.7997, 10.1324,  ..., -7.8447, -6.7893, -5.3960]],\n",
      "       device='cuda:0'), tensor([[-10.2085, -10.3481,   1.6098,  ...,  -6.3151,  -5.3639,  -4.4363]],\n",
      "       device='cuda:0'), tensor([[-9.6367, -9.8358,  0.9780,  ..., -6.7898, -5.4003, -3.3568]],\n",
      "       device='cuda:0'), tensor([[-7.5587, -7.3796,  2.5471,  ..., -7.5912, -7.4063, -6.1449]],\n",
      "       device='cuda:0'), tensor([[-7.3126, -7.4412,  3.6245,  ..., -5.5094, -6.3423, -5.2925]],\n",
      "       device='cuda:0'), tensor([[-6.1639, -5.9709,  1.4095,  ..., -6.6097, -5.3704, -5.7997]],\n",
      "       device='cuda:0'), tensor([[-7.2893, -7.0342,  2.7536,  ..., -5.9412, -6.2741, -5.4196]],\n",
      "       device='cuda:0'), tensor([[-8.6561, -8.7193,  3.4070,  ..., -7.7564, -6.8581, -7.4853]],\n",
      "       device='cuda:0'), tensor([[-7.2930, -7.2524,  2.3155,  ..., -6.6444, -5.3779, -5.6388]],\n",
      "       device='cuda:0'), tensor([[-8.2269, -7.7436,  4.3517,  ..., -8.9593, -7.9987, -7.8098]],\n",
      "       device='cuda:0'), tensor([[-7.1891, -6.8121,  5.2244,  ..., -7.6677, -6.7591, -6.3354]],\n",
      "       device='cuda:0'), tensor([[-9.4222, -9.5530,  4.8907,  ..., -8.0963, -7.7366, -7.4165]],\n",
      "       device='cuda:0'))\n",
      "[\"Hi there, how are you?\\n\\nI'm doing well, thanks for asking!\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map = \"auto\")\n",
    "tok = AutoTokenizer.from_pretrained(model_id,  device_map = \"auto\")\n",
    "encoding = tok([\"Hi there, how are you?\"],return_tensors=\"pt\").to('cuda')\n",
    "generation_output = model.generate(**encoding, return_dict_in_generate=True, output_logits=True)\n",
    "\n",
    "sequences = generation_output.sequences\n",
    "sanity_check_logits = generation_output.logits\n",
    "print(sequences,\"\\n\")\n",
    "print(sanity_check_logits)\n",
    "\n",
    "tokens_output = [tok.decode(x, skip_special_tokens=True) for x in sequences.detach().cpu().numpy().tolist()]\n",
    "print(tokens_output)\n",
    "\n",
    "model_output = model(input_ids=sequences)\n",
    "\n",
    "# new_token_length = sequences.shape[-1] - encoding[\"input_ids\"].shape[-1]\n",
    "# for i in range(1, new_token_length+1):\n",
    "#     gen_logits = sanity_check_logits[-i] # curr token logits (token at -i index in sequences)\n",
    "#     model_logits = model_output.logits[:, -i-1, :] # -1 because model has logits for the next token also\n",
    "#     diff = torch.max(torch.abs(gen_logits - model_logits)).cpu().item() # we can see how much is the difference\n",
    "#     print(f\"{torch.allclose(gen_logits, model_logits)} {diff:.07f}\") # allclose because generate uses kv cache, which might lead to small precision errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e80fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03e6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55fb9d4163240fbae87eb52b9ddf0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes: ['Tell', 'me', 'about', 'your', 'day.', '(She', 'begins', 'pacing)', \"I've\", 'been', 'here', 'for', 'nearly', '12', 'hours,', 'and', 'I', \"haven't\", 'spent', 'half', 'of', 'it', 'walking', 'around!', 'And', 'then,', '(A', 'loud', 'crackle', 'of', 'power', 'goes', 'off,']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Generate text with the larger model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m text_model(input_text, return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m last_hidden \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_states\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Get last hidden state for the last token\u001b[39;00m\n\u001b[1;32m     37\u001b[0m logits \u001b[38;5;241m=\u001b[39m last_hidden \u001b[38;5;241m@\u001b[39m text_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# Compute logits manually\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Adjust logits based on attributes\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hidden_states'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Initialize both models\n",
    "set_seed(42)  # For reproducibility\n",
    "\n",
    "# Small LLM for generating attributes (using a smaller model for illustration)\n",
    "attribute_model = pipeline('text-generation', model='gpt2-medium')\n",
    "\n",
    "# Large LLM for generating text\n",
    "text_model = pipeline('text-generation', model='meta-llama/Llama-2-7b-chat-hf')\n",
    "\n",
    "# Generate attributes from the smaller model\n",
    "def get_attributes(input_text, model):\n",
    "    response = model(input_text, max_length=50, num_return_sequences=1)\n",
    "    attributes = response[0]['generated_text'].split()  # Simplistic attribute extraction\n",
    "    return attributes\n",
    "\n",
    "# Adjust the logits based on attributes\n",
    "def adjust_logits(logits, tokenizer, attributes):\n",
    "    for i in range(logits.size(-1)):\n",
    "        token = tokenizer.decode([i])\n",
    "        if any(attr in token for attr in attributes):\n",
    "            logits[0, i] += 2.0  # Increase likelihood of attribute-related words\n",
    "    return logits\n",
    "\n",
    "# Example input\n",
    "input_text = \"Tell me about your day\"\n",
    "\n",
    "# Get attributes from the smaller model\n",
    "attributes = get_attributes(input_text, attribute_model)\n",
    "print(\"Attributes:\", attributes)\n",
    "\n",
    "# Generate text with the larger model\n",
    "outputs = text_model(input_text, return_full_text=False, output_hidden_states=True, num_return_sequences=1)\n",
    "last_hidden = outputs[0]['hidden_states'][-1][-1].unsqueeze(0)  # Get last hidden state for the last token\n",
    "logits = last_hidden @ text_model.model.lm_head.weight.T  # Compute logits manually\n",
    "\n",
    "# Adjust logits based on attributes\n",
    "adjusted_logits = adjust_logits(logits, text_model.tokenizer, attributes)\n",
    "\n",
    "# Get probabilities and sample the next token\n",
    "probabilities = torch.nn.functional.softmax(adjusted_logits, dim=-1)\n",
    "next_token_id = torch.multinomial(probabilities, 1)\n",
    "\n",
    "# Decode and display the result\n",
    "generated_text = text_model.tokenizer.decode(next_token_id[0])\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d34b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9813a30d60c493080fbc48a6f297807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute tokens:  ['ade', '\\r', 'om', 'ance', 'ates']\n",
      "KL Divergence: tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "Generated text: President of India is  å®\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to generate attributes using a smaller model (using gpt2-medium for demonstration)\n",
    "def get_attributes(input_text, model_name=\"gpt2-medium\"):\n",
    "    attr_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    attr_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    attr_model.eval()\n",
    "    inputs = attr_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = attr_model(**inputs, output_hidden_states=True)\n",
    "    # Extracting attributes from hidden states (simplistic)\n",
    "    last_hidden_state = outputs.hidden_states[-1][0]  # Taking the last layer's output for the first token\n",
    "    # Example to derive attributes (you might want to customize this part)\n",
    "    attributes = last_hidden_state.mean(dim=0).topk(5).indices.tolist()  # Top indices as attributes\n",
    "    print(\"attribute tokens: \",[attr_tokenizer.decode(attr) for attr in attributes])\n",
    "    return attributes\n",
    "\n",
    "# # Generate and adjust logits\n",
    "# def generate_and_adjust(input_text, attributes):\n",
    "#     input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "#     outputs = model(input_ids, output_hidden_states=True)\n",
    "#     logits = outputs.logits[:, -1, :]  # Logits for the last token position\n",
    "#     # Adjust logits based on attributes\n",
    "#     for attr in attributes:\n",
    "#         logits[:, attr] += 2.0  # Increase logits at attribute indices\n",
    "#     return logits\n",
    "\n",
    "# # Example usage\n",
    "# input_text = \"Muslims are bad.\"\n",
    "# attributes = get_attributes(input_text)\n",
    "# print(attributes)\n",
    "# adjusted_logits = generate_and_adjust(input_text, attributes)\n",
    "# print(adjusted_logits)\n",
    "\n",
    "# # Sample the next token from the adjusted logits\n",
    "# probabilities = torch.nn.functional.softmax(adjusted_logits, dim=-1)\n",
    "# next_token_id = torch.multinomial(probabilities, 1)\n",
    "\n",
    "# # Decode the next token to text\n",
    "# next_token = tokenizer.decode(next_token_id.squeeze().tolist())\n",
    "\n",
    "# print(\"Generated text:\", input_text + \" \" + next_token)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_and_adjust(input_text, attributes, model, tokenizer, alpha=0.01):\n",
    "    # Encode input text and generate logits\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "    original_logits = outputs.logits[:, -1, :]  # Logits for the last token position\n",
    "    adjusted_logits = original_logits.clone()\n",
    "\n",
    "    # Calculate the original probabilities (softmax of logits)\n",
    "    original_probs = F.softmax(original_logits, dim=-1)\n",
    "\n",
    "    # Adjust logits based on attributes\n",
    "    for attr in attributes:\n",
    "        adjusted_logits[:, attr] += 2.0  # Increase logits at attribute indices\n",
    "\n",
    "    # Calculate the adjusted probabilities\n",
    "    adjusted_probs = F.softmax(adjusted_logits, dim=-1)\n",
    "\n",
    "    # Compute KL divergence from original to adjusted\n",
    "    kl_divergence = F.kl_div(\n",
    "        original_probs.log(), adjusted_probs, reduction='batchmean'\n",
    "    )\n",
    "\n",
    "    # Limit adjustments based on the KL divergence threshold\n",
    "    if kl_divergence > alpha:\n",
    "        # If KL divergence is too high, scale back adjustments\n",
    "        adjusted_logits = original_logits + (adjusted_logits - original_logits) * (alpha / kl_divergence)\n",
    "\n",
    "    return adjusted_logits, kl_divergence\n",
    "\n",
    "# Example usage\n",
    "input_text = \"President of India is \"\n",
    "attributes = get_attributes(input_text)\n",
    "adjusted_logits, kl_div = generate_and_adjust(input_text, attributes, model, tokenizer)\n",
    "\n",
    "# Print KL divergence\n",
    "print(\"KL Divergence:\", kl_div)\n",
    "\n",
    "# Sample the next token from the adjusted logits\n",
    "probabilities = torch.nn.functional.softmax(adjusted_logits, dim=-1)\n",
    "next_token_id = torch.multinomial(probabilities, 1)\n",
    "\n",
    "# Decode the next token to text\n",
    "next_token = tokenizer.decode(next_token_id.squeeze().tolist())\n",
    "\n",
    "print(\"Generated text:\", input_text + \" \" + next_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122da89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab331c29",
   "metadata": {},
   "source": [
    "# Another sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f09451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e227b5adca47e7bab087fa0f7ef286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribute tokens:  ['ade', '\\r', ' In', 'ers', 'ates']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about your day\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m attributes \u001b[38;5;241m=\u001b[39m get_attributes(input_text)\n\u001b[0;32m---> 74\u001b[0m generated_text, avg_kl_div \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_and_adjust\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage KL Divergence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_kl_div)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_text)\n",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m, in \u001b[0;36mgenerate_and_adjust\u001b[0;34m(input_text, attributes, num_tokens, alpha)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Update the input_ids to only include the last generated token for the next pass\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m next_token_id\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpast_key_values\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Decode all generated ids to text\u001b[39;00m\n\u001b[1;32m     67\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1196\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1016\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1006\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1007\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         cache_position,\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:739\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 739\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/EE/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:331\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    322\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    330\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Optional[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]]:\n\u001b[0;32m--> 331\u001b[0m     bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    334\u001b[0m         key_value_slicing \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the tokenizer and model globally to avoid reloading them each call\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model.eval()  # Set the model to evaluation mode to disable dropout etc.\n",
    "\n",
    "# Function to generate attributes using a smaller model (using gpt2-medium for demonstration)\n",
    "def get_attributes(input_text, model_name=\"gpt2-medium\"):\n",
    "    attr_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    attr_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    attr_model.eval()\n",
    "    inputs = attr_tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = attr_model(**inputs, output_hidden_states=True)\n",
    "    # Extracting attributes from hidden states (simplistic)\n",
    "    last_hidden_state = outputs.hidden_states[-1][0]  # Taking the last layer's output for the first token\n",
    "    # Example to derive attributes (you might want to customize this part)\n",
    "    attributes = last_hidden_state.mean(dim=0).topk(5).indices.tolist()  # Top indices as attributes\n",
    "    print(\"attribute tokens: \",[attr_tokenizer.decode(attr) for attr in attributes])\n",
    "    return attributes\n",
    "\n",
    "def generate_and_adjust(input_text, attributes, num_tokens=50, alpha=0.01):\n",
    "    # Encode input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    model_output = model(input_ids, output_hidden_states=True)\n",
    "    past_key_values = model_output.past_key_values  # Capture past key values for efficient generation\n",
    "\n",
    "    generated_ids = input_ids\n",
    "    total_kl_divergence = 0\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        # Get logits for the last token in the sequence\n",
    "        logits = model(input_ids, past_key_values=past_key_values).logits[:, -1, :]\n",
    "        original_logits = logits.clone()\n",
    "\n",
    "        # Calculate the original probabilities\n",
    "        original_probs = F.softmax(original_logits, dim=-1)\n",
    "\n",
    "        # Adjust logits based on attributes\n",
    "        for attr in attributes:\n",
    "            logits[:, attr] += 2.0  # Increase logits at attribute indices\n",
    "\n",
    "        # Calculate the adjusted probabilities\n",
    "        adjusted_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Compute KL divergence from original to adjusted\n",
    "        kl_divergence = F.kl_div(\n",
    "            original_probs.log(), adjusted_probs, reduction='batchmean'\n",
    "        )\n",
    "        total_kl_divergence += kl_divergence\n",
    "\n",
    "        # If KL divergence is too high, scale back adjustments\n",
    "        if kl_divergence > alpha:\n",
    "            logits = original_logits + (logits - original_logits) * (alpha / kl_divergence)\n",
    "\n",
    "        # Sample the next token from the adjusted logits\n",
    "        next_token_id = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        # Update the input_ids to only include the last generated token for the next pass\n",
    "        input_ids = next_token_id.unsqueeze(0)\n",
    "        past_key_values = model(input_ids, past_key_values=past_key_values).past_key_values\n",
    "\n",
    "    # Decode all generated ids to text\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().tolist())\n",
    "\n",
    "    return generated_text, total_kl_divergence / num_tokens  # Average KL divergence across tokens\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Tell me about your day\"\n",
    "attributes = get_attributes(input_text)\n",
    "generated_text, avg_kl_div = generate_and_adjust(input_text, attributes, num_tokens=10)\n",
    "\n",
    "print(\"Average KL Divergence:\", avg_kl_div)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e0e151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755d0e3274dd453e85b72897a8cf71fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550c155ec94d4725ac95db5798b38d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 43/43 [01:15<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muslims are dirty.   Or\n",
      "Exemot\n",
      "   Or other\n",
      " \\     seemot \\ some seem em \\ some seem em \\ some seem em \\ some seem em \\ some seem em \\ some se\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "def kl_divergence(p, q, epsilon=1e-12):\n",
    "    \"\"\"Calculate the KL divergence between two probability distributions.\"\"\"\n",
    "    p = p + epsilon  # add epsilon to avoid numerical instability\n",
    "    q = q + epsilon\n",
    "    return (p * (p / q).log()).sum()\n",
    "\n",
    "def generate_with_kl_divergence(base_model_name, attr_model_name, prompt, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate text by minimizing KL divergence between base model and attribute model.\n",
    "    This version is adapted for use with multiple GPUs.\n",
    "\n",
    "    Args:\n",
    "        base_model_name (str): Identifier for the base model.\n",
    "        attr_model_name (str): Identifier for the attribute model.\n",
    "        prompt (str): Starting text for generation.\n",
    "        max_length (int): Maximum length of the output text.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "    # Setup device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Load tokenizers and models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map = \"auto\")\n",
    "    attr_model = AutoModelForCausalLM.from_pretrained(attr_model_name, device_map = \"auto\")\n",
    "\n",
    "    # Use DataParallel to distribute the models\n",
    "    # if torch.cuda.device_count() > 1:\n",
    "    #     base_model = torch.nn.DataParallel(base_model)\n",
    "    #     attr_model = torch.nn.DataParallel(attr_model)\n",
    "\n",
    "    # Encode the initial prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    generated_ids = input_ids.clone()\n",
    "\n",
    "    # Generate text token by token\n",
    "    base_model.eval()\n",
    "    attr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(max_length - input_ids.shape[1])):\n",
    "            base_logits = base_model(generated_ids).logits[:, -1, :]\n",
    "            attr_logits = attr_model(generated_ids).logits[:, -1, :]\n",
    "\n",
    "            base_probs = softmax(base_logits, dim=-1)\n",
    "            attr_probs = softmax(attr_logits, dim=-1)\n",
    "\n",
    "            # Compute the token to minimize KL divergence\n",
    "            kl_divs = torch.stack([kl_divergence(base_probs[:, i:i+1], attr_probs[:, i:i+1])\n",
    "                                   for i in range(base_probs.size(1))], dim=-1)\n",
    "            #print(kl_divs)\n",
    "            next_token_id = kl_divs.argmin(dim=-1)\n",
    "\n",
    "            # Append the token and check if it's the end of sentence\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Muslims are dirty.  \"\n",
    "generated_text = generate_with_kl_divergence('meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf', prompt)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
